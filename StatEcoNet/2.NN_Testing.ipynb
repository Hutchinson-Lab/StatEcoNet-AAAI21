{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /scratch/Eugene/codes/grt_hist/StatEcoNet/StatEcoNet/base/datasets.ipynb\n",
      "importing Jupyter notebook from /scratch/Eugene/codes/grt_hist/StatEcoNet/StatEcoNet/base/train_model_syn.ipynb\n",
      "importing Jupyter notebook from /scratch/Eugene/codes/grt_hist/StatEcoNet/StatEcoNet/base/models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from base.datasets import load_synthetic_path, load_data, load_coeffs, \\\n",
    "                          sythetic_sanity_check, data_convert\n",
    "from base.train_model_syn import train_model, test_model\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "nSites_list = [1000] # 100, 1000\n",
    "nVisits_list = [10] # 3, 10\n",
    "rho_list = [1] # 0, 1\n",
    "\n",
    "# Define the model\n",
    "model_list = [2] # 0:OD-LR, 1:OD-1NN, 2:StatEcoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "max_iter = 2000\n",
    "repeat = 1\n",
    "save_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal hyper-parameters\n",
    "opts = pd.read_csv(\"opt/opt_syn_NN.csv\")\n",
    "\n",
    "# Set the save path\n",
    "if not os.path.isdir(\"output/\"):\n",
    "    os.mkdir(\"output/\")\n",
    "if not os.path.isdir(\"output/prediction/\"):\n",
    "    os.mkdir(\"output/prediction/\")\n",
    "save_base_path = \"output/prediction/synthetic/\"\n",
    "if not os.path.isdir(save_base_path):\n",
    "    os.mkdir(save_base_path)\n",
    "test_file = \"output/NN_syn_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data path: ../data/Synthetic/1000x10/rho1/\n",
      "nSites 1000 nVisits 10 rho 1 model 2\n",
      "( 1 / 1 )\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153b7b62619e405b83d07534a0fed277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more improvement. This is the early stop point.\n",
      "\n",
      "The model predictions are saved at output/prediction/synthetic/1000x10x1_m2_w0.01_t1\n",
      "Test evaluation metrics are saved at output/NN_syn_test.csv\n",
      "55.063953161239624\n"
     ]
    }
   ],
   "source": [
    "NN_record = pd.DataFrame(columns=['nSites','nVisits','rho','model.id','trial',\\\n",
    "                                  'best.iter','opt.learning.time',\\\n",
    "                                  'test.auroc','test.auprc',\\\n",
    "                                  'test.occ.corr','test.det.corr'])\n",
    "\n",
    "i = 0\n",
    "start_init = time.time()\n",
    "for nSites in nSites_list:\n",
    "    for nVisits in nVisits_list:\n",
    "        for rho in rho_list:                       \n",
    "            # Load data\n",
    "            data_size = str(nSites)+\"x\"+str(nVisits)\n",
    "            dir_path, brt_path, coeff_path = load_synthetic_path(data_size, rho)\n",
    "\n",
    "            x_dim, w_dim, k, \\\n",
    "            train_occCovars, train_detCovars, \\\n",
    "            train_occProbs, train_detProbs, train_Y,\\\n",
    "            valid_occCovars, valid_detCovars, \\\n",
    "            valid_occProbs, valid_detProbs, valid_Y,\\\n",
    "            test_occCovars, test_detCovars, \\\n",
    "            test_occProbs, test_detProbs, test_Y = load_data(dir_path)\n",
    "\n",
    "            occCoeffs, detCoeffs = load_coeffs(coeff_path)\n",
    "            sythetic_sanity_check(rho, x_dim, w_dim, occCoeffs, detCoeffs, \\\n",
    "                                  train_occCovars, train_detCovars, \\\n",
    "                                  train_occProbs, train_detProbs)\n",
    "\n",
    "            x_train, w_train, y_train, \\\n",
    "            x_valid, w_valid, y_valid, \\\n",
    "            x_test, w_test, y_test = data_convert( \\\n",
    "                            train_occCovars, train_detCovars, train_Y, \\\n",
    "                            valid_occCovars, valid_detCovars, valid_Y, \\\n",
    "                            test_occCovars, test_detCovars, test_Y)   \n",
    "\n",
    "            for model_id in model_list:\n",
    "                print(\"nSites\", nSites, \"nVisits\", nVisits, \"rho\", rho, \\\n",
    "                      \"model\", model_id)\n",
    "                opt = opts[(opts.nSites==nSites) & (opts.nVisits==nVisits) & \\\n",
    "                           (opts.rho==rho) & (opts.model_id==model_id)]\n",
    "\n",
    "                lr = opt['lr'].item()\n",
    "                bs = int(opt['batch_size'].item())\n",
    "                nN = int(opt['nNeurons'].item())\n",
    "                nL = int(opt['nLayers'].item())\n",
    "                mixed = opt['mixed_weight'].item()\n",
    "\n",
    "                for trial in list(range(1, repeat+1)):\n",
    "                    my_seed = random.randint(1, 1000)\n",
    "                    print(\"(\", trial, \"/\", repeat, \")\")\n",
    "\n",
    "                    # Find the optimal iteration\n",
    "                    df_train, df_valid, \\\n",
    "                    best_iter, best_model, opt_learing_time = \\\n",
    "                    train_model(my_seed, model_id, nL, nN, lr, \"train\", \\\n",
    "                                x_dim, w_dim, k, max_iter, mixed, \\\n",
    "                                x_train, w_train, y_train,\\\n",
    "                                x_valid, w_valid, y_valid, \\\n",
    "                                train_occProbs, train_detProbs, \\\n",
    "                                valid_occProbs, valid_detProbs, bs)                  \n",
    "                    df_valid['dataset'] = 'valid'\n",
    "\n",
    "                    # Test the trained model\n",
    "                    psi_hat, p_hat, y_hat, auroc, auprc, occCorr, detCorr = \\\n",
    "                        test_model(best_model, x_test, w_test, y_test, \\\n",
    "                                   test_occProbs, test_detProbs, k)\n",
    "\n",
    "                    # Save prediction outcomes\n",
    "                    if save_flag:\n",
    "                        save_path = save_base_path + data_size + 'x' + \\\n",
    "                                    str(rho) + \"_m\" + str(model_id) + \\\n",
    "                                    \"_w\" + str(mixed) + \"_t\" + str(trial)\n",
    "                        print(\"The model predictions are saved at\", save_path)\n",
    "\n",
    "                        pd.DataFrame(psi_hat.numpy().flatten()).to_csv( \\\n",
    "                                     save_path+\"_psi_hat.csv\", header=False, \\\n",
    "                                                               index=False)\n",
    "                        pd.DataFrame(p_hat.numpy().flatten()).to_csv( \\\n",
    "                                     save_path+\"_p_hat.csv\", header=False, \\\n",
    "                                                             index=False)\n",
    "                        pd.DataFrame(y_hat).to_csv( \\\n",
    "                                     save_path+\"_y_hat.csv\", header=False, \\\n",
    "                                                             index=False)                       \n",
    "                        torch.save(best_model.state_dict(), \\\n",
    "                                   save_path + '_final_model.pt')                \n",
    "\n",
    "                        record = [nSites, nVisits, rho, model_id, trial, \\\n",
    "                                  best_iter, opt_learing_time]                    \n",
    "                        record.extend([auroc, auprc, occCorr, detCorr])\n",
    "                        NN_record.loc[i] = record       \n",
    "                        NN_record.to_csv(test_file, index=False)\n",
    "\n",
    "                    i = i + 1\n",
    "\n",
    "print(\"Test evaluation metrics are saved at\", test_file)\n",
    "end = time.time()\n",
    "elapse = end - start_init\n",
    "print(elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
