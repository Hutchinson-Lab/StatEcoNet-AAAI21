{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from models.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from base.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trial, model_id, nL, nN, lr, task, \\\n",
    "                x_dim, w_dim, k, n_epoch, mixed_weight, \\\n",
    "                x_train, w_train, y_train, x_te, w_te, y_te, batch_size, P=1):\n",
    "\n",
    "    # For mini batches\n",
    "    params = {'batch_size': batch_size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "    dataset = TensorDataset(x_train, w_train, y_train)\n",
    "    dataloader = DataLoader(dataset, **params)\n",
    "\n",
    "    # Model definition\n",
    "    best_iter = 0\n",
    "    torch.manual_seed(trial)\n",
    "    if model_id == 0:\n",
    "        model = OD_LR_Combined(x_dim, w_dim, k)\n",
    "    elif model_id == 1:\n",
    "        model = OD_1NN(x_dim, w_dim, k, nN)\n",
    "    elif model_id == 2:\n",
    "        occ_idx = 0\n",
    "        det_idx = 2\n",
    "        if nL == 1:\n",
    "            model = StatEcoNet_H1_Combined(x_dim, w_dim, nN, k)\n",
    "        elif nL == 3:\n",
    "            model = StatEcoNet_H3_Combined(x_dim, w_dim, nN, k)\n",
    "        else:\n",
    "            print(\"not available model\")\n",
    "            assert False\n",
    "    else:\n",
    "        print(\"not available model\")\n",
    "        assert False\n",
    "\n",
    "    # Set optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Data recorders\n",
    "    test_auprc = []\n",
    "    df_train = pd.DataFrame(columns=['lr','batchSize','nLayers','nNeurons',\\\n",
    "                                     'nIter','loss','auroc','auprc'])\n",
    "    df_test = pd.DataFrame(columns=['lr','batchSize','nLayers','nNeurons',\\\n",
    "                                    'nIter','loss','auroc','auprc'])\n",
    "\n",
    "    # Start training a model\n",
    "    test_nSite = x_te.shape[0]\n",
    "    best_iter = 0\n",
    "    best_model = 0\n",
    "    best_time = 0\n",
    "    perfect = False\n",
    "    start = time.time()\n",
    "    for i in tqdm(range(n_epoch)):\n",
    "        # for mini batches\n",
    "        train_loss = []\n",
    "        train_y_true = []\n",
    "        train_y_pred = []\n",
    "        for i_batch, xy in enumerate(dataloader):\n",
    "            # load a minibatch\n",
    "            x_tr, w_tr, y_tr = xy\n",
    "            train_y_true.extend(list(torch.flatten(y_tr).detach().numpy()))\n",
    "            train_nSite = x_tr.shape[0]\n",
    "\n",
    "            # train a model **************************************************\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            psi_hat_train, p_hat_train = model(x_tr, w_tr)\n",
    "\n",
    "            # compute training loss\n",
    "            loss = my_loss_function(y_tr, psi_hat_train, p_hat_train, \\\n",
    "                                    train_nSite, k)\n",
    "            if mixed_weight:\n",
    "                params = list(model.parameters())\n",
    "                loss += mixed_weight * \\\n",
    "                        (torch.sum(torch.norm(params[occ_idx], dim=0)))**(1/P)\n",
    "                loss += mixed_weight * \\\n",
    "                        (torch.sum(torch.norm(params[det_idx], dim=0)))**(1/P)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            # compute Y from psi_hat and p_hat\n",
    "            NN_pred = p_hat_train.reshape(p_hat_train.shape[:2]) * \\\n",
    "                      torch.cat([psi_hat_train]*k, 1)\n",
    "            NN_pred = torch.flatten(NN_pred).detach().numpy()\n",
    "            train_y_pred.extend(list(NN_pred))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        assert np.sum(train_y_true) == torch.sum(y_train)\n",
    "        # compute accuracy on train Y ****************************************\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(train_y_true, train_y_pred)\n",
    "        auroc = metrics.auc(fpr, tpr)\n",
    "        precision, recall, thresholds = \\\n",
    "                            precision_recall_curve(train_y_true, train_y_pred)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "        df_train.loc[i] = [lr, batch_size, nL, nN, i, np.mean(train_loss), \\\n",
    "                           auroc, auprc]\n",
    "\n",
    "        # evalute a model ****************************************************\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            psi_hat_test, p_hat_test = model(x_te, w_te)\n",
    "\n",
    "            # compute test loss\n",
    "            loss_t = my_loss_function(y_te, psi_hat_test, p_hat_test, \\\n",
    "                                      test_nSite, k)\n",
    "            if mixed_weight:\n",
    "                params = list(model.parameters())\n",
    "                loss_t += mixed_weight * \\\n",
    "                        (torch.sum(torch.norm(params[occ_idx], dim=0)))**(1/P)\n",
    "                loss_t += mixed_weight * \\\n",
    "                        (torch.sum(torch.norm(params[det_idx], dim=0)))**(1/P)\n",
    "\n",
    "            # compute Y from psi_hat and p_hat\n",
    "            NN_pred_test = p_hat_test.reshape(p_hat_test.shape[:2]) * \\\n",
    "                           torch.cat([psi_hat_test]*k, 1)\n",
    "            NN_pred_test = torch.flatten(NN_pred_test).detach().numpy()\n",
    "\n",
    "            # compute accuracy on test Y\n",
    "            fpr_te, tpr_te, thresholds = \\\n",
    "                        metrics.roc_curve(torch.flatten(y_te), NN_pred_test)\n",
    "            auroc_te = metrics.auc(fpr_te, tpr_te)\n",
    "            precision_te, recall_te, thresholds = \\\n",
    "                    precision_recall_curve(torch.flatten(y_te), NN_pred_test)\n",
    "            auprc_te = metrics.auc(recall_te, precision_te)      \n",
    "            test_auprc.append(auprc_te)\n",
    "\n",
    "        # CHECKING THE BEST ITERATION =================\n",
    "        if test_auprc[-1] == np.max(test_auprc):\n",
    "            best_iter = i\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_time = time.time() - start\n",
    "        else:\n",
    "            if task == \"train\" and i > best_iter + 200:\n",
    "                print(\"No more improvement. This is the early stop point.\")\n",
    "                break\n",
    "        # =============================================\n",
    "\n",
    "        df_test.loc[i] = [lr, batch_size, nL, nN, i, loss_t.item(), \\\n",
    "                          auroc_te, auprc_te]\n",
    "\n",
    "    assert best_iter < n_epoch\n",
    "\n",
    "    if task == \"train\":\n",
    "        return df_train, df_test, best_iter, best_model, best_time\n",
    "    else:\n",
    "        return df_train, df_test, model, \\\n",
    "               psi_hat_test, p_hat_test, NN_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, x_te, w_te, y_te, k):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        psi_hat_test, p_hat_test = model(x_te, w_te)\n",
    "\n",
    "        # compute Y from psi_hat and p_hat\n",
    "        NN_pred = p_hat_test.reshape(p_hat_test.shape[:2]) * \\\n",
    "                  torch.cat([psi_hat_test]*k, 1)\n",
    "        NN_pred = torch.flatten(NN_pred).detach().numpy()\n",
    "\n",
    "        # compute accuracy on Y\n",
    "        fpr, tpr, thresholds = \\\n",
    "                            metrics.roc_curve(torch.flatten(y_te), NN_pred)\n",
    "        auroc = metrics.auc(fpr, tpr)\n",
    "        precision, recall, thresholds = \\\n",
    "                        precision_recall_curve(torch.flatten(y_te), NN_pred)\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "        return(psi_hat_test, p_hat_test, NN_pred, auroc, auprc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
